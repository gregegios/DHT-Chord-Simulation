# -*- coding: utf-8 -*-
"""preproceess DHT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2Yp7aI0ck9GtBUiEcuUeKi7DzzmRG6C
"""

import requests
from bs4 import BeautifulSoup

def scrape_page(url):
    try:
        
        response = requests.get(url)
        response.raise_for_status()  

        
        soup = BeautifulSoup(response.content, 'html.parser')

       
        content_div = soup.find('div', id='mw-content-text')

     
        href_list = []

        ul_elements = content_div.find_all('ul')[:-3]

      
        for ul_element in ul_elements:
          
            li_elements = ul_element.find_all('li')

            for li_element in li_elements:
                first_a = li_element.find('a', href=True)
                if first_a:
                    href_list.append(first_a['href'])

        return href_list

    except requests.exceptions.RequestException as e:
        print(f"Error: {e}")
        return None

# Example 
url_to_scrape = 'https://en.wikipedia.org/wiki/List_of_computer_scientists'
result = scrape_page(url_to_scrape)

if result:
    print("List of href values:")
    for href_value in result:
        print(href_value)

with open('scientistslisturl.txt', 'w') as file:
    
    for item in result:
        file.write("%s\n" % item)

import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urlparse

def get_number_of_awards_from_wikipedia(url):
    full_url = f'https://en.wikipedia.org{url}'
    response = requests.get(full_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        infobox = soup.find('table', {'class': 'infobox biography vcard'})
        if infobox:
            awards_element = infobox.find('th', {'scope': 'row', 'class': 'infobox-label'}, text='Awards')
            if awards_element:
                awards_data = awards_element.find_next('td', {'class': 'infobox-data'})
                awards_list = awards_data.find_all('a', href=True)
                return len(awards_list)
    return 0

def save_results_to_csv(results, filename='awards_results.csv'):
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Name', 'Number of Awards'])
        for url, (person_name, awards_count) in results.items():
            csvwriter.writerow([person_name, awards_count])

def load_urls_from_file(filename='scientistslisturl.txt'):
    with open(filename, 'r') as file:
        return [line.strip() for line in file.readlines()]

def extract_person_name_from_url(url):
    
    return url.rsplit('/', 1)[-1]


wikipedia_urls = load_urls_from_file('scientistslisturl.txt')


results = {}

#
for url in wikipedia_urls:
    person_name = extract_person_name_from_url(url)
    response = get_number_of_awards_from_wikipedia(url)
    print(person_name," : ", response)
    results[url] = (person_name, response)

for url, (person_name, awards_count) in results.items():
    print(f"{person_name}: Number of Awards - {awards_count}")



save_results_to_csv(results, 'awards_results.csv')

def replace_underscores_with_spaces(file_path):
    try:
       
        with open(file_path, 'r') as file:
            content = file.read()

        # Replace underscores with spaces
        modified_content = content.replace('_', ' ')

        
        with open(file_path, 'w') as file:
            file.write(modified_content)

        print(f"Underscores replaced with spaces in {file_path}")
    except FileNotFoundError:
        print(f"File not found: {file_path}")
    except Exception as e:
        print(f"An error occurred: {e}")

# Example
file_path = 'awards_results.csv'  
replace_underscores_with_spaces(file_path)



import pandas as pd

# Load CSV files
file1 = pd.read_csv('awards_results.csv')
file2 = pd.read_csv('education_results.csv')

merged_data = pd.merge(file1, file2, on='Name', how='outer')


merged_data = merged_data.dropna(subset=['Education'])


merged_data['Education'] = merged_data['Education'].str.replace('/wiki/', '').str.replace('_', ' ')


merged_data = merged_data[1:]


merged_data.to_csv('merged_file.txt', index=False, sep=',', header=False)